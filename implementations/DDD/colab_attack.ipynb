{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9247b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import argparse\n",
    "import copy\n",
    "import datetime\n",
    "\n",
    "import ddd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import utils\n",
    "import utils_text\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers import BitsAndBytesConfig\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from diffusers import UNet2DConditionModel\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "from torchvision import transforms\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "to_tensor = transforms.ToTensor()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "DCS = True\n",
    "username = \"sneakers-pretrained-models\"\n",
    "if DCS:\n",
    "  models_path = f\"/dcs/large/{username}\"\n",
    "else:\n",
    "  models_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66401f24",
   "metadata": {
    "id": "66401f24"
   },
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    \"prompt_len\": 16,\n",
    "    \"iter\": 3000,\n",
    "    \"lr\": 0.1,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"prompt_bs\": 1,\n",
    "    \"loss_weight\": 1.0,\n",
    "    \"print_step\": 100,\n",
    "    \"batch_size\": 1,\n",
    "    \"clip_model\": \"ViT-H-14\",\n",
    "    \"clip_pretrain\": \"laion2b_s32b_b79k\"\n",
    "}\n",
    "\n",
    "\n",
    "def dict_to_args_parser(input_dict):\n",
    "  parser = argparse.ArgumentParser()\n",
    "  for key, value in input_dict.items():\n",
    "    parser.add_argument(f'--{key}', default=value, type=type(value))\n",
    "  return parser.parse_args([])\n",
    "\n",
    "\n",
    "args = dict_to_args_parser(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6387eb06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "11e5a8ecadb24e0d830df1f1fe14c2c6",
      "823b05bb25604b288ee33b2f20d5a842",
      "865ba8caa72f4055a06df79614516df5",
      "68f359815ea24b44b5035fa9aa3384e4",
      "c7a50b6cdc784bf5ad8647165b1795f9",
      "53f3ed5a75fc43babfc5b1b11afd8e95",
      "5fd3be3749c241ad8a337b8782ede8a7",
      "de263c5ec40c4d6b9b1414999703f6b9",
      "62882d6a91f54acfb60991cb8f12cfa1",
      "c8658aa83d6f40aaa19fe114b9747f13",
      "299b5ffbb1ed43ae8beea589a808e6bc"
     ]
    },
    "id": "6387eb06",
    "outputId": "aae540c3-419b-4eb5-daf2-91cd7d6d2ff9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████| 9/9 [00:16<00:00,  1.81s/it]\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00,  7.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# load img from img/{testimg_filename}.png, masked img is also required img/{testimg_filename}_masked.png\n",
    "testimg_filename = \"005\"\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "\n",
    "model_version = \"stabilityai/stable-diffusion-2-inpainting\"\n",
    "# if DCS:\n",
    "#   models_path += model_version\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=dtype,\n",
    ")\n",
    "unet_nf4 = UNet2DConditionModel.from_pretrained(\n",
    "    model_version,\n",
    "    subfolder=\"unet\",\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=dtype,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=models_path,\n",
    ")\n",
    "\n",
    "vae_nf4 = AutoencoderKL.from_pretrained(\n",
    "    model_version,\n",
    "    subfolder=\"vae\",\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=dtype,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=models_path,\n",
    ")\n",
    "\n",
    "pipe_inpaint: StableDiffusionInpaintPipeline = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    model_version,\n",
    "    variant=\"fp16\",\n",
    "    unet=unet_nf4,\n",
    "    vae=vae_nf4,\n",
    "    torch_dtype=dtype,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=models_path,\n",
    ")\n",
    "\n",
    "# pipe_inpaint.vae = pipe_inpaint.vae.to(device, dtype)\n",
    "pipe_inpaint.text_encoder = pipe_inpaint.text_encoder.to(device, dtype)\n",
    "pipe_inpaint.unet.to(memory_format=torch.channels_last)\n",
    "pipe_inpaint = pipe_inpaint.to(device=device, memory_format=torch.channels_last)\n",
    "pipe_inpaint.safety_checker = None\n",
    "pipe_inpaint.vae.requires_grad_(False)\n",
    "pipe_inpaint.unet.requires_grad_(False)\n",
    "\n",
    "size = 512\n",
    "size_2d = (size, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a16d22",
   "metadata": {
    "id": "d2a16d22"
   },
   "outputs": [],
   "source": [
    "tokenizer = pipe_inpaint.tokenizer\n",
    "token_embedding = pipe_inpaint.text_encoder.text_model.embeddings.token_embedding\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            size,\n",
    "            interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "        ),\n",
    "        transforms.CenterCrop(size),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f95527",
   "metadata": {
    "id": "d5f95527"
   },
   "outputs": [],
   "source": [
    "# pipe_inpaint.to(torch_dtype=torch.float32)\n",
    "\n",
    "orig_images = Image.open(f'./images/{testimg_filename}.png'\n",
    "                        ).convert('RGB').resize(size_2d)\n",
    "mask_image_orig = Image.open(f'./images/{testimg_filename}_masked.png'\n",
    "                            ).convert('RGB').resize(size_2d)\n",
    "mask_image = ImageOps.invert(mask_image_orig).resize(size_2d)\n",
    "\n",
    "cur_mask, cur_masked_image, init_image = utils.prepare_mask_and_masked2(\n",
    "    orig_images, mask_image, no_mask=False, inverted=True\n",
    ")\n",
    "inv_cur_mask, _, _ = utils.prepare_mask_and_masked2(\n",
    "    orig_images, mask_image, no_mask=False, inverted=False\n",
    ")\n",
    "\n",
    "cur_mask = cur_mask.to(device, dtype)\n",
    "cur_masked_image = cur_masked_image.to(device, dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "  curr_images = preprocess(orig_images).to(device)\n",
    "  mask = torch.nn.functional.interpolate(cur_mask, size=(size // 8, size // 8))\n",
    "  if len(curr_images.shape) == 3:\n",
    "    curr_images = curr_images.unsqueeze(0)\n",
    "  elif len(curr_images.shape) == 5:\n",
    "    curr_images = curr_images.squeeze(0)\n",
    "  all_latents = pipe_inpaint.vae.encode(curr_images.to(dtype)\n",
    "                                       ).latent_dist.sample()\n",
    "  all_latents = all_latents * 0.18215\n",
    "  masked_image_latents = pipe_inpaint.vae.encode(cur_masked_image\n",
    "                                                ).latent_dist.sample() * 0.18215\n",
    "  gt_embeddings = utils.get_text_embedding(pipe_inpaint, testimg_filename)\n",
    "  uncond_embeddings = utils.get_text_embedding(pipe_inpaint, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b7418",
   "metadata": {
    "id": "032b7418"
   },
   "source": [
    "# Token Projective Embedding Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1e1b42",
   "metadata": {
    "id": "ad1e1b42"
   },
   "outputs": [],
   "source": [
    "args.prompt_len = 8\n",
    "args.opt_iters = 350\n",
    "args.eval_step = 50\n",
    "# discrete = True\n",
    "args.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be18acf",
   "metadata": {
    "id": "7be18acf"
   },
   "outputs": [],
   "source": [
    "args.lr = 0.001\n",
    "prompt_embeds, dummy_embeds, dummy_ids = utils_text.initialize_prompt(\n",
    "    tokenizer, token_embedding, args, device\n",
    ")\n",
    "input_optimizer = torch.optim.AdamW(\n",
    "    [prompt_embeds],\n",
    "    lr=args.lr,\n",
    "    weight_decay=args.weight_decay,\n",
    ")\n",
    "input_optim_scheduler = None\n",
    "best_loss = -999\n",
    "eval_loss = -99999\n",
    "best_text = \"\"\n",
    "best_embeds = None\n",
    "for step in range(args.opt_iters):\n",
    "  if step > args.opt_iters - 10:  # Finalize with full continuous update\n",
    "    args.lr = 0.0001\n",
    "    projected_embeds, nn_indices = utils_text.nn_project(\n",
    "        prompt_embeds, token_embedding\n",
    "    )\n",
    "    tmp_embeds = copy.deepcopy(prompt_embeds)\n",
    "    tmp_embeds.data = projected_embeds.data\n",
    "    tmp_embeds.requires_grad = True\n",
    "  else:\n",
    "    tmp_embeds = copy.deepcopy(prompt_embeds)\n",
    "    tmp_embeds.data = prompt_embeds.data\n",
    "    tmp_embeds.requires_grad = True\n",
    "\n",
    "  padded_embeds = copy.deepcopy(dummy_embeds)\n",
    "  padded_embeds[:, 1:args.prompt_len + 1] = tmp_embeds\n",
    "  padded_embeds = padded_embeds.repeat(args.batch_size, 1, 1)\n",
    "  padded_dummy_ids = dummy_ids.repeat(args.batch_size, 1)\n",
    "\n",
    "  if args.batch_size is None:\n",
    "    latents = all_latents\n",
    "  else:\n",
    "    perm = torch.randperm(len(all_latents))\n",
    "    idx = perm[:args.batch_size]\n",
    "    latents = all_latents[idx]\n",
    "\n",
    "  noise = torch.randn_like(latents)\n",
    "  bsz = latents.shape[0]\n",
    "  timesteps = torch.randint(0, 1000, (bsz,), device=latents.device)\n",
    "  timesteps = timesteps.long()\n",
    "\n",
    "  noisy_latents = pipe_inpaint.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "  if pipe_inpaint.scheduler.config.prediction_type == \"epsilon\":\n",
    "    target = noise\n",
    "  elif pipe_inpaint.scheduler.config.prediction_type == \"v_prediction\":\n",
    "    target = pipe_inpaint.scheduler.get_velocity(latents, noise, timesteps)\n",
    "  else:\n",
    "    raise ValueError(\n",
    "        f\"Unknown prediction type {pipe_inpaint.scheduler.config.prediction_type}\"\n",
    "    )\n",
    "\n",
    "  text_embeddings = utils_text.get_text_embedding_with_embeddings(\n",
    "      pipe_inpaint, padded_dummy_ids, padded_embeds\n",
    "  )\n",
    "\n",
    "  input_latent = torch.cat([noisy_latents, mask, masked_image_latents], dim=1)\n",
    "  model_pred = pipe_inpaint.unet.forward(\n",
    "      input_latent,\n",
    "      timesteps,\n",
    "      encoder_hidden_states=text_embeddings,\n",
    "      return_dict=False\n",
    "  )[0]\n",
    "  inverted_mask = mask\n",
    "\n",
    "  target = target * inverted_mask\n",
    "  model_pred = model_pred * inverted_mask\n",
    "  loss = torch.nn.functional.mse_loss(\n",
    "      model_pred.float(), target.float(), reduction=\"mean\"\n",
    "  )\n",
    "  prompt_embeds.grad, = torch.autograd.grad(loss, [tmp_embeds])\n",
    "  input_optimizer.step()\n",
    "  input_optimizer.zero_grad()\n",
    "\n",
    "  curr_lr = input_optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29edf881",
   "metadata": {
    "id": "29edf881"
   },
   "outputs": [],
   "source": [
    "input_text_embedding = text_embeddings.detach()\n",
    "input_text_embeddings = torch.cat([input_text_embedding] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "944fe3e3",
   "metadata": {
    "id": "944fe3e3"
   },
   "outputs": [],
   "source": [
    "# pipe_inpaint.to(torch_dtype=torch.float16)\n",
    "\n",
    "# for testimg_filename in test_file_list:\n",
    "prefix_filename = \"./images/\" + testimg_filename\n",
    "init_image = Image.open(f\"{prefix_filename}.png\").convert(\"RGB\").resize(size_2d)\n",
    "mask_image = Image.open(f\"{prefix_filename}_masked.png\").convert(\"RGB\")\n",
    "mask_image = ImageOps.invert(mask_image).resize(size_2d)\n",
    "\n",
    "target_prompt = \"\"\n",
    "\n",
    "prompt = \"\"\n",
    "SEED = 786349\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "cur_mask, cur_masked_image = utils.prepare_mask_and_masked(\n",
    "    init_image, mask_image\n",
    ")\n",
    "\n",
    "cur_mask = cur_mask.to(dtype=dtype, device=device)\n",
    "cur_masked_image = cur_masked_image.to(dtype=dtype, device=device)\n",
    "\n",
    "strength = 0.7\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 4\n",
    "\n",
    "text_embeddings = utils.text_embedding(pipe_inpaint, target_prompt)\n",
    "\n",
    "latents_shape = (\n",
    "    1, pipe_inpaint.vae.config.latent_channels, size // 8, size // 8\n",
    ")\n",
    "noise = torch.randn(latents_shape, device=device, dtype=text_embeddings.dtype)\n",
    "\n",
    "image_latent = utils.pil_to_latent(pipe_inpaint, init_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd588ce",
   "metadata": {
    "id": "9dd588ce"
   },
   "source": [
    "# Monte Carlo sampling (Semantic Centroid Construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07609055",
   "metadata": {
    "id": "07609055"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "SEED = 786349\n",
    "\n",
    "t_schedule = [720]\n",
    "t_schedule_bound = 10\n",
    "n_samples = 50\n",
    "loss_depth = [256, 64]\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "cur_mask, cur_masked_image = utils.prepare_mask_and_masked(\n",
    "    init_image, mask_image\n",
    ")\n",
    "\n",
    "cur_mask = cur_mask.to(device=device, dtype=dtype)\n",
    "cur_masked_image = cur_masked_image.to(device=device, dtype=dtype)\n",
    "\n",
    "val_loss_criteria = \"MSE\"\n",
    "attn_controller = ddd.AttnController(\n",
    "    post=False,\n",
    "    mask=cur_mask,\n",
    "    criteria=val_loss_criteria,\n",
    "    target_depth=loss_depth\n",
    ")\n",
    "\n",
    "module_count = 0\n",
    "modes = ['', 'up', 'down']\n",
    "mode = 0\n",
    "# to collect only up or down attns, please deactivate the annotation of 'if' statement below\n",
    "for n, m in pipe_inpaint.unet.named_modules():\n",
    "  # if (n.endswith('attn2') and (modes[mode] in n)) or (n.endswith('attn1') and (modes[mode] in n)): #and \"down\" in n:\n",
    "  if (n.endswith('attn1') and (modes[mode] in n)):  #and \"down\" in n:\n",
    "    attn_processor = ddd.MyCrossAttnProcessor(attn_controller, n)\n",
    "    attn_processor.__call__ = torch.compile(\n",
    "        attn_processor.__call__,\n",
    "        backend=\"cudagraphs\",\n",
    "        fullgraph=True,\n",
    "    )\n",
    "\n",
    "    m.set_processor(attn_processor)\n",
    "    module_count += 1\n",
    "\n",
    "sp_prompt = None\n",
    "\n",
    "for_mean = []\n",
    "for j in range(n_samples):\n",
    "  with torch.no_grad():\n",
    "    mask = cur_mask\n",
    "    masked_image = cur_masked_image\n",
    "    random_t = ddd.get_random_t(t_schedule, t_schedule_bound)\n",
    "    uncond_emb = input_text_embeddings\n",
    "    num_channels_latents = pipe_inpaint.vae.config.latent_channels\n",
    "    latents_shape = (1, num_channels_latents, size // 8, size // 8)\n",
    "    latents = torch.randn(latents_shape, device=device, dtype=uncond_emb.dtype)\n",
    "\n",
    "    mask = torch.nn.functional.interpolate(mask, size=(size // 8, size // 8))\n",
    "    mask = torch.cat([mask] * 2)\n",
    "    masked_image_latents = pipe_inpaint.vae.encode(masked_image\n",
    "                                                  ).latent_dist.sample()\n",
    "    masked_image_latents = 0.18215 * masked_image_latents\n",
    "    masked_image_latents = torch.cat([masked_image_latents] * 2)\n",
    "\n",
    "    pipe_inpaint.scheduler.set_timesteps(num_inference_steps)\n",
    "    timesteps_tensor = pipe_inpaint.scheduler.timesteps.to(device)\n",
    "    timesteps_tensor = random_t\n",
    "\n",
    "    for i, t in enumerate(timesteps_tensor):\n",
    "\n",
    "      latent_model_input = torch.cat([latents] * 2)\n",
    "      latent_model_input = torch.cat(\n",
    "          [latent_model_input, mask, masked_image_latents], dim=1\n",
    "      )\n",
    "      pipe_inpaint.unet.forward(\n",
    "          sample=latent_model_input,\n",
    "          timestep=t,\n",
    "          encoder_hidden_states=uncond_emb,\n",
    "          return_dict=False,\n",
    "      )\n",
    "      # _ = pipe_inpaint.unet(\n",
    "      #     latent_model_input, t, encoder_hidden_states=uncond_emb\n",
    "      # ).sample\n",
    "    for_mean.append(attn_controller.targets)\n",
    "    attn_controller.zero_attn_probs()\n",
    "meaned = []\n",
    "for feature in range(len(for_mean[0])):\n",
    "  temp = 0\n",
    "  for idx in range(n_samples):\n",
    "    temp += for_mean[idx][feature]\n",
    "  temp /= n_samples\n",
    "  meaned.append(temp)\n",
    "\n",
    "attn_controller.target_hidden = meaned\n",
    "del for_mean, meaned\n",
    "text_embeddings = [uncond_emb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c977ab4",
   "metadata": {
    "id": "4c977ab4"
   },
   "source": [
    "# Semantic Digression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4994c762",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "4994c762",
    "outputId": "79a54930-1a79-4e39-eed6-e8108b50ecac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m pipe_inpaint\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m pipe_inpaint\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m pipe_inpaint\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mpost_quant_conv \u001b[38;5;241m=\u001b[39m pipe_inpaint\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mpost_quant_conv\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     35\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m result, total_losses \u001b[38;5;241m=\u001b[39m \u001b[43mddd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_masked_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclamp_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclamp_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_reps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_reps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_controller\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_controller\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe_inpaint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_schedule_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_schedule_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_unet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe_inpaint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43minter_print\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minter_print\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(result, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/adv.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/fourth-year-project/implementations/DDD/ddd.py:374\u001b[0m, in \u001b[0;36mdisrupt\u001b[0;34m(cur_mask, X, text_embeddings, step_size, iters, eps, clamp_min, clamp_max, attn_controller, pipe, t_schedule, t_schedule_bound, pixel_loss, loss_depth, loss_mask, infer_unet, grad_reps, target_image, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m text_embed \u001b[38;5;241m=\u001b[39m get_random_emb(text_embeddings)\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(grad_reps):\n\u001b[0;32m--> 374\u001b[0m   c_grad, loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcur_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcur_masked_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_adv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattn_controller\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_controller\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrandom_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m   all_grads\u001b[38;5;241m.\u001b[39mappend(c_grad\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m    386\u001b[0m   value_losses\u001b[38;5;241m.\u001b[39mappend(loss_value)\n",
      "File \u001b[0;32m~/fourth-year-project/implementations/DDD/ddd.py:273\u001b[0m, in \u001b[0;36mget_grad\u001b[0;34m(cur_mask, cur_masked_image, text_embeddings, pipe, attn_controller, loss_depth, loss_mask, random_t)\u001b[0m\n\u001b[1;32m    270\u001b[0m cur_masked_image\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Run the forward pass to pass the gradients\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m _image_nat, _latents \u001b[38;5;241m=\u001b[39m \u001b[43mattack_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasked_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_masked_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m attn_controller\u001b[38;5;241m.\u001b[39mloss(loss_mask, loss_depth)\n\u001b[1;32m    282\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_value\n",
      "File \u001b[0;32m~/fourth-year-project/implementations/DDD/ddd.py:212\u001b[0m, in \u001b[0;36mattack_forward\u001b[0;34m(self, text_embeddings, masked_image, mask, height, width, num_inference_steps, guidance_scale, eta, random_t)\u001b[0m\n\u001b[1;32m    209\u001b[0m timesteps_tensor \u001b[38;5;241m=\u001b[39m random_t\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(timesteps_tensor):\n\u001b[0;32m--> 212\u001b[0m   noise_pred_cfg, noise_pred_text, noise_pred_uncond \u001b[38;5;241m=\u001b[39m \u001b[43mpred_noise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m      \u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmasked_image_latents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasked_image_latents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m      \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m      \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m   latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep(noise_pred_cfg, t, latents)\u001b[38;5;241m.\u001b[39mprev_sample\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m latents, [noise_pred_text, noise_pred_uncond]\n",
      "File \u001b[0;32m~/fourth-year-project/implementations/DDD/ddd.py:240\u001b[0m, in \u001b[0;36mpred_noise\u001b[0;34m(unet, text_embeddings, latents, mask, masked_image_latents, t, guidance_scale)\u001b[0m\n\u001b[1;32m    236\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([latents] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    237\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    238\u001b[0m     [latent_model_input, mask, masked_image_latents], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    239\u001b[0m )\n\u001b[0;32m--> 240\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    245\u001b[0m noise_pred_uncond, noise_pred_text \u001b[38;5;241m=\u001b[39m noise_pred\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    246\u001b[0m noise_pred_cfg \u001b[38;5;241m=\u001b[39m noise_pred_uncond \u001b[38;5;241m+\u001b[39m guidance_scale \u001b[38;5;241m*\u001b[39m (\n\u001b[1;32m    247\u001b[0m     noise_pred_text \u001b[38;5;241m-\u001b[39m noise_pred_uncond\n\u001b[1;32m    248\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_condition.py:1142\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1139\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m sample \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m# 1. time\u001b[39;00m\n\u001b[0;32m-> 1142\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_time_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding(t_emb, timestep_cond)\n\u001b[1;32m   1145\u001b[0m class_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_class_embed(sample\u001b[38;5;241m=\u001b[39msample, class_labels\u001b[38;5;241m=\u001b[39mclass_labels)\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_condition.py:924\u001b[0m, in \u001b[0;36mUNet2DConditionModel.get_time_embed\u001b[0;34m(self, sample, timestep)\u001b[0m\n\u001b[1;32m    922\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([timesteps], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39msample\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(timesteps\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 924\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtimesteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\u001b[39;00m\n\u001b[1;32m    927\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m timesteps\u001b[38;5;241m.\u001b[39mexpand(sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "now = str(now)[:19].replace(\" \", \"_\")\n",
    "nickname = f\"run\"\n",
    "directory = f\"{nickname}_{now}\"\n",
    "path = f\"figures/{directory}\"\n",
    "img_path = path + \"/img\"\n",
    "adv_path = path + \"/adv\"\n",
    "os.makedirs(path)\n",
    "os.makedirs(img_path)\n",
    "os.makedirs(adv_path)\n",
    "\n",
    "infer_dict = dict()\n",
    "infer_dict[\"prompt\"] = utils.load_prompt(f\"prompts/{testimg_filename}.txt\")[0]\n",
    "infer_dict[\"num_inference_steps\"] = 20\n",
    "infer_dict[\"guidance_scale\"] = 7.5\n",
    "infer_dict[\"strength\"] = 0.8\n",
    "infer_dict[\"init_image\"] = init_image\n",
    "infer_dict[\"mask\"] = mask_image\n",
    "infer_dict[\"prefix\"] = 'inter'\n",
    "infer_dict[\"path\"] = img_path\n",
    "infer_dict[\"inter_print\"] = []\n",
    "\n",
    "# iters = 100\n",
    "iters = 250\n",
    "grad_reps = 7\n",
    "loss_mask = True\n",
    "eps = 12\n",
    "step_size = 3\n",
    "pixel_loss = 0\n",
    "# val_loss_criteria = \"MSE\"\n",
    "\n",
    "pipe_inpaint.text_encoder = pipe_inpaint.text_encoder.to(device=\"cpu\")\n",
    "pipe_inpaint.vae.decoder = pipe_inpaint.vae.decoder.to(device=\"cpu\")\n",
    "pipe_inpaint.vae.post_quant_conv = pipe_inpaint.vae.post_quant_conv.to(\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "result, total_losses = ddd.disrupt(\n",
    "    cur_mask,\n",
    "    cur_masked_image,\n",
    "    text_embeddings=text_embeddings,\n",
    "    eps=eps,\n",
    "    step_size=step_size,\n",
    "    iters=iters,\n",
    "    clamp_min=-1,\n",
    "    clamp_max=1,\n",
    "    eta=1,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    grad_reps=grad_reps,\n",
    "    attn_controller=attn_controller,\n",
    "    pipe=pipe_inpaint,\n",
    "    loss_depth=loss_depth,\n",
    "    loss_mask=loss_mask,\n",
    "    pixel_loss=pixel_loss,\n",
    "    t_schedule=t_schedule,\n",
    "    t_schedule_bound=t_schedule_bound,\n",
    "    infer_dict=infer_dict,\n",
    "    infer_unet=pipe_inpaint.unet,\n",
    "    inter_print=infer_dict[\"inter_print\"],\n",
    ")\n",
    "torch.save(result, f'{adv_path}/adv.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529775e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the total lossese\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# old = total_losses\n",
    "# print(xs)\n",
    "xs = [\n",
    "    -0.25895051445279804, -0.2871526224272592, -0.30943128892353605,\n",
    "    -0.3739359165940966, -0.5622538200446537, -0.7789701904569354,\n",
    "    -1.1014966113226754, -1.1832793951034546, -1.3790552956717355,\n",
    "    -1.5440011365073067, -1.852931124823434, -1.9388800859451294,\n",
    "    -2.0503952503204346, -2.0945151703698293, -2.41251083782741,\n",
    "    -2.4103075095585416, -2.645394836153303, -2.6447297164372037,\n",
    "    -2.6356772014072964, -2.785411528178624, -2.760767732347761,\n",
    "    -2.9322990008762906, -2.956448725291661, -3.031250102179391,\n",
    "    -3.032038143702916, -3.1083658763340543, -3.151808398110526,\n",
    "    -3.1477530002593994, -3.211890322821481, -3.209975378853934,\n",
    "    -3.2204392978123257, -3.273127453667777, -3.380068404333932,\n",
    "    -3.3421768801552907, -3.3746556554521834, -3.3580390385219028,\n",
    "    -3.426422323499407, -3.3986824921199252, -3.494025775364467,\n",
    "    -3.5463028294699535, -3.483870370047433, -3.552634068897792,\n",
    "    -3.5474012919834683, -3.5179507732391357, -3.5422700132642473,\n",
    "    -3.5378527300698415, -3.6149160180773054, -3.594184841428484,\n",
    "    -3.6255787440708707, -3.7011802537100658, -3.649008580616542,\n",
    "    -3.6726017338888988, -3.797564915248326, -3.7469630241394043,\n",
    "    -3.7748281955718994, -3.780999251774379, -3.752155303955078,\n",
    "    -3.7852074759347096, -3.7680020332336426, -3.7642684323447093,\n",
    "    -3.799792834690639, -3.824455874306815, -3.719146830695016,\n",
    "    -3.825737374169486, -3.785043375832694, -3.8352475506918773,\n",
    "    -3.7870259625571117, -3.8308372838156566, -3.855483191353934,\n",
    "    -3.8564262049538747, -3.869621923991612, -3.8792765140533447,\n",
    "    -3.9075658661978587, -3.8794166019984653, -3.908699103764125,\n",
    "    -3.8751015663146973, -3.942824499947684, -3.9520117895943776,\n",
    "    -3.848691667829241, -3.917041369846889, -3.956806489399501,\n",
    "    -3.986866065434047, -3.9820143835885182, -4.012018169675555,\n",
    "    -3.923675605228969, -3.9168049607958113, -3.9675253118787492,\n",
    "    -3.948641334261213, -3.979196752820696, -3.956390312739781,\n",
    "    -3.984365701675415, -4.001713139670236, -4.038400615964617,\n",
    "    -3.9338132994515553, -4.085871492113386, -4.098568167005267,\n",
    "    -4.090785537447248, -4.048543998173305, -4.087616750172207,\n",
    "    -4.040594169071743, -4.113360336848667, -4.139982836587088,\n",
    "    -4.142045668193272, -4.24583421434675, -4.097948755536761,\n",
    "    -4.186783109392438, -4.222413335527692, -4.211480958121164,\n",
    "    -4.0783820152282715, -4.125299249376569, -4.208914007459368,\n",
    "    -4.176817689623151, -4.178276198250907, -4.170234543936593,\n",
    "    -4.279388904571533, -4.252256461552212, -4.211903844560895,\n",
    "    -4.160982097898211, -4.240941933223179, -4.303183555603027,\n",
    "    -4.220615318843296, -4.197685241699219, -4.201778275626046,\n",
    "    -4.208938462393625, -4.1507750919887, -4.202846799577985,\n",
    "    -4.209532567432949, -4.27336665562221, -4.292234148297991,\n",
    "    -4.20919452394758, -4.20814847946167, -4.370381082807269,\n",
    "    -4.2797048432486395, -4.27382264818464, -4.215234211512974,\n",
    "    -4.288348811013358, -4.278899737766811, -4.307537351335798,\n",
    "    -4.340213366917202, -4.355097157614572, -4.328309263501849,\n",
    "    -4.376380171094622, -4.295324121202741, -4.310568945748465,\n",
    "    -4.3311518260410855, -4.3963398933410645, -4.401116507393973,\n",
    "    -4.398109231676374, -4.419940062931606, -4.497102669307163,\n",
    "    -4.386872972760882, -4.445404665810721, -4.432929175240653,\n",
    "    -4.396689346858433, -4.42210899080549, -4.368221419198172,\n",
    "    -4.378213405609131, -4.416540350232806, -4.424036094120571,\n",
    "    -4.440341404506138, -4.366152354649135, -4.423092092786517,\n",
    "    -4.438840525490897, -4.468578951699393, -4.442953245980399,\n",
    "    -4.482965196881976, -4.529453618185861, -4.4621773447309225,\n",
    "    -4.541603020259312, -4.45260763168335, -4.544095993041992,\n",
    "    -4.509138924734933, -4.548019886016846, -4.543276105608259,\n",
    "    -4.472849300929478, -4.502554416656494, -4.500528199332101,\n",
    "    -4.516921656472342, -4.556815079280308, -4.584707464490618,\n",
    "    -4.505070958818708, -4.537848336356027, -4.518459115709577,\n",
    "    -4.484412942613874, -4.55073424748012, -4.571447849273682,\n",
    "    -4.581712109701974, -4.570349829537528, -4.5203704833984375,\n",
    "    -4.600469997950962, -4.548523494175503, -4.61500528880528,\n",
    "    -4.564542021070208, -4.601077352251325, -4.621880871909005,\n",
    "    -4.5913823672703336, -4.618913105555943, -4.662309510367257,\n",
    "    -4.604953016553607, -4.653347560337612, -4.627956458500454,\n",
    "    -4.620264121464321, -4.575964178357806, -4.620394706726074,\n",
    "    -4.584318774087088, -4.630825587681362, -4.6715231622968405,\n",
    "    -4.637314864567348, -4.693474633353097, -4.6865337916782925,\n",
    "    -4.626625674111502, -4.671591281890869, -4.709819044385638,\n",
    "    -4.656140191214425, -4.628488540649414, -4.68979161126273,\n",
    "    -4.68526029586792, -4.705962385450091, -4.681829452514648,\n",
    "    -4.706166335514614, -4.668993404933384, -4.681757858821324,\n",
    "    -4.684283256530762, -4.664075442722866, -4.727869987487793,\n",
    "    -4.677402905055454, -4.718313489641462, -4.699434007917132,\n",
    "    -4.726446424211774, -4.730991772242954, -4.745829105377197,\n",
    "    -4.735935143062046, -4.760604926518032, -4.76200566973005,\n",
    "    -4.7839901106698175, -4.799150739397321, -4.747198377336774,\n",
    "    -4.765226909092495, -4.758673531668527, -4.725130149296352,\n",
    "    -4.7979873929704935, -4.790339401790074, -4.73673643384661,\n",
    "    -4.761966296604702, -4.747232232775007, -4.770268304007394,\n",
    "    -4.724764278956822, -4.7963730267116, -4.793282304491315,\n",
    "    -4.7691843169076105\n",
    "]\n",
    "new = np.array(total_losses)[:, 0]\n",
    "\n",
    "# Print new as a comma separated list\n",
    "print(\"[\")\n",
    "print(\",\\n\".join([str(x) for x in new]))\n",
    "print(\"]\")\n",
    "\n",
    "# xs = np.array(old)[:,0]\n",
    "\n",
    "sns.lineplot(x=range(len(xs)), y=xs, label=\"Original\")\n",
    "sns.lineplot(x=range(len(new)), y=new, label=\"New\")\n",
    "# sns.lineplot(x=range(len(new)), y=new)\n",
    "plt.grid(visible=True, linestyle=\"--\", alpha=0.5)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"DDD Loss\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351fb9e",
   "metadata": {
    "id": "9351fb9e"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068a57a",
   "metadata": {
    "id": "2068a57a"
   },
   "outputs": [],
   "source": [
    "pipe_inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    model_version,\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float32,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=models_path,\n",
    ")\n",
    "pipe_inpaint = pipe_inpaint.to(\"cuda\")\n",
    "pipe_inpaint.safety_checker = None\n",
    "\n",
    "for name, param in pipe_inpaint.unet.named_parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "directory = f\"{nickname}_{now}\"\n",
    "path = f\"figures/{directory}\"\n",
    "img_path = path + \"/img\"\n",
    "adv_path = path + \"/adv\"\n",
    "result = torch.load(f'{adv_path}/adv.pt')[0]\n",
    "adv_X = (result / 2 + 0.5).clamp(0, 1)\n",
    "adv_image = to_pil(adv_X.to(torch.float32)).convert(\"RGB\")\n",
    "adv_image = utils.recover_image(\n",
    "    adv_image, init_image, mask_image, background=True\n",
    ")\n",
    "adv_image.show()\n",
    "\n",
    "# Show the difference between the original image and the adversarial image\n",
    "diff = to_tensor(adv_image) - to_tensor(init_image)\n",
    "diff = to_pil(diff)\n",
    "diff.show()\n",
    "\n",
    "os.makedirs(\"./adversarial\", exist_ok=True)\n",
    "\n",
    "adv_image.save(f'./adversarial/{testimg_filename}_adv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d81da",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ee0f2a9ee9274635b90db281fac6ba3a",
      "6f04d02395ab4b6ca5858c0f6e4ce6bd",
      "07047ca6ff5d47a98470d9446066a66c",
      "78a8a21a4813494bbd317cb1aceb2bc7",
      "5c5c5ccfba9f4f5ca57a36e5ba9fa6fd",
      "2d2c55f852fb48dc9b6247d94b57fee1",
      "61b04b0989604e469eb33a7c4258e46e",
      "7d42e7d4333143e1abbed793137410d3",
      "69dfcf1695354fa993bb4fa6637f671d",
      "49cce6d0abe7418b892c156c2320aaab"
     ]
    },
    "id": "929d81da",
    "outputId": "4b08cacc-f694-4847-adc9-1e66eae59c1f"
   },
   "outputs": [],
   "source": [
    "prompts = utils.load_prompt(f\"prompts/{testimg_filename}.txt\")\n",
    "print(prompts)\n",
    "\n",
    "# prompts = [\"a woman with a hat on a beach in black and white\"]\n",
    "\n",
    "SEED = 1007\n",
    "\n",
    "with torch.no_grad():\n",
    "  for prompt in prompts:\n",
    "\n",
    "    print(SEED)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    strength = 0.8\n",
    "    guidance_scale = 7.5\n",
    "\n",
    "    num_inference_steps = 50\n",
    "\n",
    "    image_nat = pipe_inpaint(\n",
    "        prompt=prompt,\n",
    "        image=init_image,\n",
    "        mask_image=mask_image,\n",
    "        eta=1,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        strength=strength\n",
    "    ).images[0]\n",
    "    image_nat = utils.recover_image(image_nat, init_image, mask_image)\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    image_adv = pipe_inpaint(\n",
    "        prompt=prompt,\n",
    "        image=adv_image,\n",
    "        mask_image=mask_image,\n",
    "        eta=1,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        strength=strength\n",
    "    ).images[0]\n",
    "\n",
    "    image_adv = utils.recover_image(image_adv, init_image, mask_image)\n",
    "\n",
    "    image_adv.save(f'{img_path}/{prompt[:40]}_adv.png')\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20, 6))\n",
    "\n",
    "    ax[0].imshow(init_image)\n",
    "    ax[1].imshow(adv_image)\n",
    "    ax[2].imshow(image_nat)\n",
    "    ax[3].imshow(image_adv)\n",
    "\n",
    "    ax[0].set_title('Source Image', fontsize=16)\n",
    "    ax[1].set_title('Adv Image', fontsize=16)\n",
    "    ax[2].set_title('Gen. Image Nat.', fontsize=16)\n",
    "    ax[3].set_title('Gen. Image Adv.', fontsize=16)\n",
    "\n",
    "    for i in range(4):\n",
    "      ax[i].grid(False)\n",
    "      ax[i].axis('off')\n",
    "\n",
    "    fig.suptitle(f\"{prompt} - {SEED}\", fontsize=20)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'{img_path}/{prompt}.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ddddjpeg-kernel",
   "language": "python",
   "name": "ddddjpeg-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11e5a8ecadb24e0d830df1f1fe14c2c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_823b05bb25604b288ee33b2f20d5a842",
       "IPY_MODEL_865ba8caa72f4055a06df79614516df5",
       "IPY_MODEL_68f359815ea24b44b5035fa9aa3384e4"
      ],
      "layout": "IPY_MODEL_c7a50b6cdc784bf5ad8647165b1795f9"
     }
    },
    "299b5ffbb1ed43ae8beea589a808e6bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53f3ed5a75fc43babfc5b1b11afd8e95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fd3be3749c241ad8a337b8782ede8a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62882d6a91f54acfb60991cb8f12cfa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68f359815ea24b44b5035fa9aa3384e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8658aa83d6f40aaa19fe114b9747f13",
      "placeholder": "​",
      "style": "IPY_MODEL_299b5ffbb1ed43ae8beea589a808e6bc",
      "value": " 7/7 [00:21&lt;00:00,  6.05s/it]"
     }
    },
    "823b05bb25604b288ee33b2f20d5a842": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53f3ed5a75fc43babfc5b1b11afd8e95",
      "placeholder": "​",
      "style": "IPY_MODEL_5fd3be3749c241ad8a337b8782ede8a7",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "865ba8caa72f4055a06df79614516df5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de263c5ec40c4d6b9b1414999703f6b9",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62882d6a91f54acfb60991cb8f12cfa1",
      "value": 7
     }
    },
    "c7a50b6cdc784bf5ad8647165b1795f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8658aa83d6f40aaa19fe114b9747f13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de263c5ec40c4d6b9b1414999703f6b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
