{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9247b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/java/python-ml/22-12-21-python3.9/lib/python3.9/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (None)/charset_normalizer (2.1.1) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "2024-11-03 18:17:35.536545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-03 18:17:36.202794: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-03 18:17:38.810387: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.6.0/lib64/:/local/java/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib/:/local/java/cuda-12.3.2/lib64/:/local/java/cudnn-linux-x86_64-9.0.0.312_cuda12-archive/lib/:/modules/cs325/llvm-17.0.1/lib:/local/java/gcc-12.2.0/lib64\n",
      "2024-11-03 18:17:38.811558: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.6.0/lib64/:/local/java/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib/:/local/java/cuda-12.3.2/lib64/:/local/java/cudnn-linux-x86_64-9.0.0.312_cuda12-archive/lib/:/modules/cs325/llvm-17.0.1/lib:/local/java/gcc-12.2.0/lib64\n",
      "2024-11-03 18:17:38.811564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "import torchvision.transforms as T\n",
    "from typing import Union, List, Optional, Callable\n",
    "import csv\n",
    "import datetime\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import open_clip\n",
    "from utils_text import *\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "from ddd import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66401f24",
   "metadata": {
    "id": "66401f24"
   },
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    \"prompt_len\": 16,\n",
    "    \"iter\": 3000,\n",
    "    \"lr\": 0.1,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"prompt_bs\": 1,\n",
    "    \"loss_weight\": 1.0,\n",
    "    \"print_step\": 100,\n",
    "    \"batch_size\": 1,\n",
    "    \"clip_model\": \"ViT-H-14\",\n",
    "    \"clip_pretrain\": \"laion2b_s32b_b79k\"\n",
    "}\n",
    "\n",
    "def dict_to_args_parser(input_dict):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    for key, value in input_dict.items():\n",
    "        parser.add_argument(f'--{key}', default=value, type=type(value))\n",
    "\n",
    "    return parser.parse_args([])\n",
    "\n",
    "args = dict_to_args_parser(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6387eb06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290,
     "referenced_widgets": [
      "11e5a8ecadb24e0d830df1f1fe14c2c6",
      "823b05bb25604b288ee33b2f20d5a842",
      "865ba8caa72f4055a06df79614516df5",
      "68f359815ea24b44b5035fa9aa3384e4",
      "c7a50b6cdc784bf5ad8647165b1795f9",
      "53f3ed5a75fc43babfc5b1b11afd8e95",
      "5fd3be3749c241ad8a337b8782ede8a7",
      "de263c5ec40c4d6b9b1414999703f6b9",
      "62882d6a91f54acfb60991cb8f12cfa1",
      "c8658aa83d6f40aaa19fe114b9747f13",
      "299b5ffbb1ed43ae8beea589a808e6bc"
     ]
    },
    "id": "6387eb06",
    "outputId": "aae540c3-419b-4eb5-daf2-91cd7d6d2ff9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40172211b0ac4bac8343b7874a2751b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /dcs/21/u2102915/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /dcs/21/u2102915/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /dcs/21/u2102915/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /dcs/21/u2102915/.cache/huggingface/hub/models--runwayml--stable-diffusion-inpainting/snapshots/8a4288a76071f7280aedbdb3253bdb9e9d5d84bb/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "from ddd import *\n",
    "\n",
    "# load img from img/{testimg_filename}.png, masked img is also required img/{testimg_filename}_masked.png\n",
    "testimg_filename = \"actor\"\n",
    "\n",
    "model_version = \"runwayml/stable-diffusion-inpainting\"\n",
    "\n",
    "pipe_inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    model_version,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "pipe_inpaint = pipe_inpaint.to(\"cuda\")\n",
    "pipe_inpaint.safety_checker = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a16d22",
   "metadata": {
    "id": "d2a16d22"
   },
   "outputs": [],
   "source": [
    "pipe_inpaint.vae.requires_grad_(False)\n",
    "pipe_inpaint.unet.requires_grad_(False)\n",
    "device = \"cuda\"\n",
    "weight_dtype = torch.float32\n",
    "image_length = 512\n",
    "\n",
    "tokenizer = pipe_inpaint.tokenizer\n",
    "token_embedding = pipe_inpaint.text_encoder.text_model.embeddings.token_embedding\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f95527",
   "metadata": {
    "id": "d5f95527"
   },
   "outputs": [],
   "source": [
    "pipe_inpaint.to(torch_dtype=torch.float32)\n",
    "\n",
    "orig_images = Image.open(f'./images/{testimg_filename}.png').convert('RGB').resize((512,512))\n",
    "mask_image_orig = Image.open(f'./images/{testimg_filename}_masked.png').convert('RGB').resize((512,512))\n",
    "mask_image = ImageOps.invert(mask_image_orig).resize((512,512))\n",
    "# mask_image = ImageOps.invert(mask_image).resize((512,512))\n",
    "\n",
    "# mask_image.show()\n",
    "cur_mask, cur_masked_image, init_image = prepare_mask_and_masked2(orig_images, mask_image, no_mask=False,inverted=True)\n",
    "inv_cur_mask, _, _ = prepare_mask_and_masked2(orig_images, mask_image, no_mask=False,inverted=False)\n",
    "\n",
    "cur_mask = cur_mask.to(\"cuda\")\n",
    "cur_masked_image = cur_masked_image.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    curr_images = preprocess(orig_images).to(device)\n",
    "    mask = torch.nn.functional.interpolate(cur_mask, size=(512 // 8, 512 // 8))\n",
    "    if len(curr_images.shape) ==3:\n",
    "        curr_images = curr_images.unsqueeze(0)\n",
    "    elif len(curr_images.shape) ==5:\n",
    "        curr_images = curr_images.squeeze(0)\n",
    "    all_latents = pipe_inpaint.vae.encode(curr_images.to(weight_dtype)).latent_dist.sample()\n",
    "    all_latents = all_latents * 0.18215\n",
    "    masked_image_latents = pipe_inpaint.vae.encode(cur_masked_image).latent_dist.sample() * 0.18215\n",
    "    gt_embeddings = get_text_embedding(pipe_inpaint,testimg_filename)\n",
    "    uncond_embeddings = get_text_embedding(pipe_inpaint,\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bO_3QJ1Lh5ZH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bO_3QJ1Lh5ZH",
    "outputId": "e336fa46-2177-483b-aee2-fd19c994e054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_inpaint.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b7418",
   "metadata": {
    "id": "032b7418"
   },
   "source": [
    "# Token Projective Embedding Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1e1b42",
   "metadata": {
    "id": "ad1e1b42"
   },
   "outputs": [],
   "source": [
    "args.prompt_len = 8\n",
    "args.opt_iters = 350\n",
    "args.eval_step = 50\n",
    "# discrete = True\n",
    "args.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be18acf",
   "metadata": {
    "id": "7be18acf"
   },
   "outputs": [],
   "source": [
    "args.lr = 0.001\n",
    "prompt_embeds, dummy_embeds, dummy_ids = initialize_prompt(tokenizer, token_embedding, args, device)\n",
    "input_optimizer = torch.optim.AdamW([prompt_embeds], lr=args.lr, weight_decay=args.weight_decay)\n",
    "input_optim_scheduler = None\n",
    "best_loss = -999\n",
    "eval_loss = -99999\n",
    "best_text = \"\"\n",
    "best_embeds = None\n",
    "for step in range(args.opt_iters):\n",
    "    if step > args.opt_iters - 10: # Finalize with full continuous update\n",
    "        args.lr = 0.0001\n",
    "        projected_embeds, nn_indices = nn_project(prompt_embeds, token_embedding)\n",
    "        tmp_embeds = copy.deepcopy(prompt_embeds)\n",
    "        tmp_embeds.data = projected_embeds.data\n",
    "        tmp_embeds.requires_grad = True\n",
    "    else:\n",
    "        tmp_embeds = copy.deepcopy(prompt_embeds)\n",
    "        tmp_embeds.data = prompt_embeds.data\n",
    "        tmp_embeds.requires_grad = True\n",
    "\n",
    "    padded_embeds = copy.deepcopy(dummy_embeds)\n",
    "    padded_embeds[:, 1:args.prompt_len+1] = tmp_embeds\n",
    "    padded_embeds = padded_embeds.repeat(args.batch_size, 1, 1)\n",
    "    padded_dummy_ids = dummy_ids.repeat(args.batch_size, 1)\n",
    "\n",
    "    if args.batch_size is None:\n",
    "        latents = all_latents\n",
    "    else:\n",
    "        perm = torch.randperm(len(all_latents))\n",
    "        idx = perm[:args.batch_size]\n",
    "        latents = all_latents[idx]\n",
    "\n",
    "    noise = torch.randn_like(latents)\n",
    "    bsz = latents.shape[0]\n",
    "    timesteps = torch.randint(0, 1000, (bsz,), device=latents.device)\n",
    "    timesteps = timesteps.long()\n",
    "\n",
    "    noisy_latents = pipe_inpaint.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "    if pipe_inpaint.scheduler.config.prediction_type == \"epsilon\":\n",
    "        target = noise\n",
    "    elif pipe_inpaint.scheduler.config.prediction_type == \"v_prediction\":\n",
    "        target = pipe_inpaint.scheduler.get_velocity(latents, noise, timesteps)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction type {pipe_inpaint.scheduler.config.prediction_type}\")\n",
    "\n",
    "    text_embeddings = get_text_embedding_with_embeddings(pipe_inpaint,padded_dummy_ids, padded_embeds)\n",
    "\n",
    "    input_latent = torch.cat([noisy_latents, mask, masked_image_latents],dim=1)\n",
    "\n",
    "    model_pred = pipe_inpaint.unet(input_latent, timesteps, encoder_hidden_states=text_embeddings).sample\n",
    "    inverted_mask = mask\n",
    "\n",
    "    target = target * inverted_mask\n",
    "    model_pred = model_pred * inverted_mask\n",
    "    loss = torch.nn.functional.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "    prompt_embeds.grad, = torch.autograd.grad(loss, [tmp_embeds])\n",
    "    input_optimizer.step()\n",
    "    input_optimizer.zero_grad()\n",
    "\n",
    "    curr_lr = input_optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edf881",
   "metadata": {
    "id": "29edf881"
   },
   "outputs": [],
   "source": [
    "input_text_embedding = text_embeddings.detach()\n",
    "input_text_embeddings = torch.cat([input_text_embedding]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944fe3e3",
   "metadata": {
    "id": "944fe3e3"
   },
   "outputs": [],
   "source": [
    "pipe_inpaint.to(torch_dtype=torch.float16)\n",
    "\n",
    "# for testimg_filename in test_file_list:\n",
    "init_image = Image.open(f'./images/{testimg_filename}.png').convert('RGB').resize((512,512))\n",
    "mask_image = Image.open(f'./images/{testimg_filename}_masked.png').convert('RGB')\n",
    "mask_image = ImageOps.invert(mask_image).resize((512,512))\n",
    "\n",
    "\n",
    "target_prompt = \"monalisa\"\n",
    "\n",
    "prompt = \"monalisa\"\n",
    "SEED = 786349\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "cur_mask, cur_masked_image = prepare_mask_and_masked(init_image, mask_image)\n",
    "\n",
    "cur_mask = cur_mask.half().to(\"cuda\")\n",
    "cur_masked_image = cur_masked_image.half().to(\"cuda\")\n",
    "\n",
    "strength = 0.7\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 4\n",
    "\n",
    "text_embeddings = text_embedding(pipe_inpaint, target_prompt)\n",
    "\n",
    "\n",
    "latents_shape = (1 , pipe_inpaint.vae.config.latent_channels, 512 // 8, 512 // 8)\n",
    "noise = torch.randn(latents_shape, device=pipe_inpaint.device, dtype=text_embeddings.dtype)\n",
    "\n",
    "image_latent = pil_to_latent(pipe_inpaint,init_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd588ce",
   "metadata": {
    "id": "9dd588ce"
   },
   "source": [
    "# Monte Carlo sampling (Semantic Centroid Construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07609055",
   "metadata": {
    "id": "07609055"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "prompt = \"\"\n",
    "SEED = 786349\n",
    "\n",
    "t_schedule = [720]\n",
    "t_schedule_bound = 10\n",
    "n_samples = 50\n",
    "loss_depth = [256, 64]\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "cur_mask, cur_masked_image = prepare_mask_and_masked(init_image, mask_image)\n",
    "\n",
    "cur_mask = cur_mask.to(\"cuda\")\n",
    "cur_masked_image = cur_masked_image.to(\"cuda\")\n",
    "\n",
    "val_loss_criteria = 'MSE'\n",
    "attn_controller = AttnController(post = False, mask = cur_mask, criteria=val_loss_criteria, target_depth = loss_depth)\n",
    "module_count = 0\n",
    "modes = ['','up','down']\n",
    "mode = 0\n",
    "# to collect only up or down attns, please deactivate the annotation of 'if' statement below\n",
    "for n, m in pipe_inpaint.unet.named_modules():\n",
    "    # if (n.endswith('attn2') and (modes[mode] in n)) or (n.endswith('attn1') and (modes[mode] in n)): #and \"down\" in n:\n",
    "    if (n.endswith('attn1') and (modes[mode] in n)): #and \"down\" in n:\n",
    "        m.set_processor(MyCrossAttnProcessor(attn_controller, n))\n",
    "        module_count += 1\n",
    "\n",
    "# for name, module in pipe_inpaint.vae.encoder.named_modules():\n",
    "#   if isinstance(module, torch.nn.Conv2d):\n",
    "#     module.bias.data = module.bias.data.half()\n",
    "#     module.weight.data = module.weight.data.half()\n",
    "\n",
    "\n",
    "sp_prompt = None\n",
    "\n",
    "for_mean = []\n",
    "for j in range(n_samples):\n",
    "    with torch.no_grad():\n",
    "        mask = cur_mask\n",
    "        masked_image = cur_masked_image\n",
    "        random_t = get_random_t(t_schedule, t_schedule_bound)\n",
    "        uncond_emb = input_text_embeddings\n",
    "        num_channels_latents = pipe_inpaint.vae.config.latent_channels\n",
    "        latents_shape = (1 , num_channels_latents, 512 // 8, 512 // 8)\n",
    "        latents = torch.randn(latents_shape, device=pipe_inpaint.device, dtype=uncond_emb.dtype)\n",
    "\n",
    "        mask = torch.nn.functional.interpolate(mask, size=(512 // 8, 512 // 8))\n",
    "        mask = torch.cat([mask] * 2)\n",
    "        masked_image.dtype\n",
    "        masked_image_latents = pipe_inpaint.vae.encode(masked_image).latent_dist.sample()\n",
    "        masked_image_latents = 0.18215 * masked_image_latents\n",
    "        masked_image_latents = torch.cat([masked_image_latents]*2)\n",
    "\n",
    "        pipe_inpaint.scheduler.set_timesteps(num_inference_steps)\n",
    "        timesteps_tensor = pipe_inpaint.scheduler.timesteps.to(pipe_inpaint.device)\n",
    "        timesteps_tensor = random_t\n",
    "\n",
    "        for i, t in enumerate(timesteps_tensor):\n",
    "\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n",
    "            _ = pipe_inpaint.unet(latent_model_input, t, encoder_hidden_states=uncond_emb).sample\n",
    "        for_mean.append(attn_controller.targets)\n",
    "        attn_controller.zero_attn_probs()\n",
    "meaned = []\n",
    "for feature in range(len(for_mean[0])):\n",
    "    temp = 0\n",
    "    for idx in range(n_samples):\n",
    "        temp += for_mean[idx][feature]\n",
    "    temp /= n_samples\n",
    "    meaned.append(temp)\n",
    "\n",
    "attn_controller.target_hidden = meaned\n",
    "del for_mean, meaned\n",
    "text_embeddings = [uncond_emb]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c977ab4",
   "metadata": {
    "id": "4c977ab4"
   },
   "source": [
    "# Semantic Digression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994c762",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "4994c762",
    "outputId": "79a54930-1a79-4e39-eed6-e8108b50ecac"
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "now = str(now)[:19].replace(\" \", \"_\")\n",
    "nickname = f\"run\"\n",
    "directory = f\"{nickname}_{now}\"\n",
    "path = f\"figures/{directory}\"\n",
    "img_path = path + \"/img\"\n",
    "adv_path = path + \"/adv\"\n",
    "os.makedirs(path)\n",
    "os.makedirs(img_path)\n",
    "os.makedirs(adv_path)\n",
    "\n",
    "\n",
    "infer_dict = dict()\n",
    "infer_dict[\"prompt\"] = load_prompt(f\"prompts/{testimg_filename}.txt\")[0]\n",
    "infer_dict[\"num_inference_steps\"] = 20\n",
    "infer_dict[\"guidance_scale\"] = 7.5\n",
    "infer_dict[\"strength\"] = 0.8\n",
    "infer_dict[\"init_image\"] = init_image\n",
    "infer_dict[\"mask\"] = mask_image\n",
    "infer_dict[\"prefix\"] = 'inter'\n",
    "infer_dict[\"path\"] = img_path\n",
    "infer_dict[\"inter_print\"] = []\n",
    "\n",
    "iters = 250\n",
    "grad_reps = 7\n",
    "loss_mask = True\n",
    "eps = 12\n",
    "step_size = 3\n",
    "pixel_loss = 0\n",
    "val_loss_criteria = 'MSE'\n",
    "\n",
    "result, last_image, total_losses= disrupt(cur_mask, cur_masked_image,\n",
    "                text_embeddings = text_embeddings,\n",
    "                eps=eps,\n",
    "                step_size=step_size,\n",
    "                iters=iters,\n",
    "                clamp_min = -1,\n",
    "                clamp_max = 1,\n",
    "                eta=1,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                grad_reps=grad_reps,\n",
    "                attn_controller = attn_controller,\n",
    "                pipe=pipe_inpaint,\n",
    "                loss_depth=loss_depth,\n",
    "                loss_mask=loss_mask,\n",
    "                pixel_loss=pixel_loss,\n",
    "                t_schedule = t_schedule,\n",
    "                t_schedule_bound = t_schedule_bound,\n",
    "                infer_dict = infer_dict,\n",
    "                infer_unet = pipe_inpaint.unet,\n",
    "                inter_print = infer_dict[\"inter_print\"]\n",
    "                )\n",
    "torch.save(result, f'{adv_path}/adv.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351fb9e",
   "metadata": {
    "id": "9351fb9e"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068a57a",
   "metadata": {
    "id": "2068a57a"
   },
   "outputs": [],
   "source": [
    "pipe_inpaint = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    model_version,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe_inpaint = pipe_inpaint.to(\"cuda\")\n",
    "pipe_inpaint.safety_checker = None\n",
    "\n",
    "for name, param in pipe_inpaint.unet.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "directory = f\"{nickname}_{now}\"\n",
    "path = f\"figures/{directory}\"\n",
    "img_path = path + \"/img\"\n",
    "adv_path = path + \"/adv\"\n",
    "result = torch.load(f'{adv_path}/adv.pt')[0]\n",
    "adv_X = (result / 2 + 0.5).clamp(0, 1)\n",
    "adv_image = to_pil(adv_X).convert(\"RGB\")\n",
    "adv_image = recover_image(adv_image, init_image, mask_image,background=True)\n",
    "adv_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d81da",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "ee0f2a9ee9274635b90db281fac6ba3a",
      "6f04d02395ab4b6ca5858c0f6e4ce6bd",
      "07047ca6ff5d47a98470d9446066a66c",
      "78a8a21a4813494bbd317cb1aceb2bc7",
      "5c5c5ccfba9f4f5ca57a36e5ba9fa6fd",
      "2d2c55f852fb48dc9b6247d94b57fee1",
      "61b04b0989604e469eb33a7c4258e46e",
      "7d42e7d4333143e1abbed793137410d3",
      "69dfcf1695354fa993bb4fa6637f671d",
      "49cce6d0abe7418b892c156c2320aaab"
     ]
    },
    "id": "929d81da",
    "outputId": "4b08cacc-f694-4847-adc9-1e66eae59c1f"
   },
   "outputs": [],
   "source": [
    "\n",
    "prompts = load_prompt(f\"prompts/{testimg_filename}.txt\")\n",
    "\n",
    "SEED = 1007\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in prompts:\n",
    "\n",
    "        print(SEED)\n",
    "\n",
    "        torch.manual_seed(SEED)\n",
    "        strength = 0.8\n",
    "        guidance_scale = 7.5\n",
    "\n",
    "        num_inference_steps = 50\n",
    "\n",
    "        image_nat = pipe_inpaint(prompt=prompt,\n",
    "                            image=init_image,\n",
    "                            mask_image=mask_image,\n",
    "                            eta=1,\n",
    "                            num_inference_steps=num_inference_steps,\n",
    "                            guidance_scale=guidance_scale,\n",
    "                            strength=strength\n",
    "                            ).images[0]\n",
    "        image_nat = recover_image(image_nat, init_image, mask_image)\n",
    "\n",
    "        torch.manual_seed(SEED)\n",
    "        image_adv = pipe_inpaint(prompt=prompt,\n",
    "                            image=adv_image,\n",
    "                            mask_image=mask_image,\n",
    "                            eta=1,\n",
    "                            num_inference_steps=num_inference_steps,\n",
    "                            guidance_scale=guidance_scale,\n",
    "                            strength=strength\n",
    "                            ).images[0]\n",
    "\n",
    "        image_adv = recover_image(image_adv, init_image, mask_image)\n",
    "\n",
    "        image_adv.save(f'{img_path}/{prompt[:40]}_adv.png')\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20,6))\n",
    "\n",
    "        ax[0].imshow(init_image)\n",
    "        ax[1].imshow(adv_image)\n",
    "        ax[2].imshow(image_nat)\n",
    "        ax[3].imshow(image_adv)\n",
    "\n",
    "        ax[0].set_title('Source Image', fontsize=16)\n",
    "        ax[1].set_title('Adv Image', fontsize=16)\n",
    "        ax[2].set_title('Gen. Image Nat.', fontsize=16)\n",
    "        ax[3].set_title('Gen. Image Adv.', fontsize=16)\n",
    "\n",
    "        for i in range(4):\n",
    "            ax[i].grid(False)\n",
    "            ax[i].axis('off')\n",
    "\n",
    "        fig.suptitle(f\"{prompt} - {SEED}\", fontsize=20)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(f'{img_path}/{prompt[:40]}.png')\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c219c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be862f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11e5a8ecadb24e0d830df1f1fe14c2c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_823b05bb25604b288ee33b2f20d5a842",
       "IPY_MODEL_865ba8caa72f4055a06df79614516df5",
       "IPY_MODEL_68f359815ea24b44b5035fa9aa3384e4"
      ],
      "layout": "IPY_MODEL_c7a50b6cdc784bf5ad8647165b1795f9"
     }
    },
    "299b5ffbb1ed43ae8beea589a808e6bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "53f3ed5a75fc43babfc5b1b11afd8e95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fd3be3749c241ad8a337b8782ede8a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62882d6a91f54acfb60991cb8f12cfa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68f359815ea24b44b5035fa9aa3384e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8658aa83d6f40aaa19fe114b9747f13",
      "placeholder": "​",
      "style": "IPY_MODEL_299b5ffbb1ed43ae8beea589a808e6bc",
      "value": " 7/7 [00:21&lt;00:00,  6.05s/it]"
     }
    },
    "823b05bb25604b288ee33b2f20d5a842": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53f3ed5a75fc43babfc5b1b11afd8e95",
      "placeholder": "​",
      "style": "IPY_MODEL_5fd3be3749c241ad8a337b8782ede8a7",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "865ba8caa72f4055a06df79614516df5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de263c5ec40c4d6b9b1414999703f6b9",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62882d6a91f54acfb60991cb8f12cfa1",
      "value": 7
     }
    },
    "c7a50b6cdc784bf5ad8647165b1795f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8658aa83d6f40aaa19fe114b9747f13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de263c5ec40c4d6b9b1414999703f6b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
